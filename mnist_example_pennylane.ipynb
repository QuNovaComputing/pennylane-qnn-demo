{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports all required libraries. We use pytorch to define and train neural networks as well as provide the classical layers. We use torchvision to load the MNIST digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch dataset loading, model definition, and model training code\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# pennylane imports for defining the quantum part of the model\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as np\n",
    "\n",
    "\n",
    "# for timing the training process\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we handle loading the MNIST digits and optionally select a reduced set of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: /home/rpj/pytorch_data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "\n",
      "Test data:\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: /home/rpj/pytorch_data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "# load the MNIST training and testing datasets\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"~/pytorch_data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"~/pytorch_data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# allow for restricting classes to specific digits\n",
    "from functools import reduce\n",
    "def restrict_classes(dataset, classes):\n",
    "    classes_membership_mask = reduce(lambda a,b: a|b,\n",
    "                                     [dataset.targets == i for i in classes],\n",
    "                                     torch.zeros(dataset.targets.size(), dtype=int))\n",
    "    idx = torch.where(classes_membership_mask)\n",
    "    dataset.data = dataset.data[idx]\n",
    "    dataset.targets = dataset.targets[idx]\n",
    "    return dataset\n",
    "\n",
    "# NOTE: you can change this list to select which digit classes are\n",
    "# used from the datasets.\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "print(\"Train data:\")\n",
    "print(restrict_classes(training_data, classes))\n",
    "print()\n",
    "print(\"Test data:\")\n",
    "print(restrict_classes(test_data, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the hybrid model. We first define the quantum part, then the hybrid model class that makes use of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we set simulator defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: here, we configure the hyperparameters of the quantum layer.\n",
    "# You must install the lightning.qubit device for cpu runs or\n",
    "# lightning.gpu device for gpu runs. You can also set use_lightning=False\n",
    "# to use the slow default.qubit backend.\n",
    "nqubits = 4\n",
    "nlayers = 2\n",
    "\n",
    "use_lightning = False\n",
    "use_gpu = False\n",
    "# default to cpu pytorch ops\n",
    "torch_device = \"cpu\"\n",
    "if use_lightning:\n",
    "    if use_gpu:\n",
    "        qml_device = qml.device('lightning.gpu', wires=nqubits)\n",
    "        # override to use gpu in pytorch\n",
    "        torch_device = \"cuda\"\n",
    "    else:\n",
    "        qml_device = qml.device('lightning.qubit', wires=nqubits)\n",
    "else:\n",
    "    if use_gpu:\n",
    "        raise RuntimeError(\"Cannot use gpu without also using lightning simulator.\")\n",
    "    qml_device = qml.device('qulacs.simulator', wires=nqubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set use_ibmq to True to override defaults and run the circuits on IBM devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_ibmq = True\n",
    "if use_ibmq:\n",
    "    # Set your IBMQ API key here, or in the pennylane config file (see https://docs.pennylane.ai/en/stable/introduction/configuration.html)\n",
    "    ibmq_api_key = \"\"\n",
    "    if ibmq_api_key == \"\":\n",
    "        try:\n",
    "            ibmq_api_key = qml.default_config[\"qiskit.ibmq.ibmqx_token\"]\n",
    "        except:\n",
    "            raise ValueError(\"You need to set your IBMQ api key in the variable ibmq_api_key or the config file.\")\n",
    "    \n",
    "    # Pick the real device name here.\n",
    "    # Some options are \"ibm_brisbane\", \"ibm_kyoto\", \"ibm_osaka\".\n",
    "    # A simulator such as \"simulator_statevector\" or \"simulator_mps\" can also be selected.\n",
    "    ibmq_device_name = \"ibm_brisbane\"\n",
    "    qml_device = qml.device(\"qiskit.ibmq\", ibmqx_token=ibmq_api_key, backend=ibmq_device_name, wires=nqubits)\n",
    "    print(f\"Successfully set up to run on real device {ibmq_device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the quantum part of the model\n",
    "@qml.qnode(qml_device, interface=\"torch\")\n",
    "def qnn_layer(inputs, weights):\n",
    "    # encode inputs from previous layer\n",
    "    # as rotations.\n",
    "    for i in range(nqubits):\n",
    "        qml.RX(inputs[i], wires=i)\n",
    "    # place gates using the trainable weights\n",
    "    # as parameters.\n",
    "    for layer_index in range(nlayers):\n",
    "        # place the trainable rotations\n",
    "        for i in range(nqubits):\n",
    "            qml.RY(weights[i + layer_index * nqubits], wires=i)\n",
    "        # place the entangling gates\n",
    "        for i in range(nqubits):\n",
    "            j = (i + 1) % nqubits\n",
    "            qml.CNOT(wires=(i, j))\n",
    "    # now, return the pauli Z expectation values\n",
    "    # on each qubit.\n",
    "    return tuple(qml.expval(qml.PauliZ(i)) for i in range(nqubits))\n",
    "\n",
    "\n",
    "class HybridClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # to convert image arrays to vectors\n",
    "        self.flatten = nn.Flatten()\n",
    "        # to reduce number of features for input to the qnn\n",
    "        self.reduction_layer = nn.Linear(28*28, nqubits)\n",
    "        # to map the qnn outputs to a class (some values are unused if\n",
    "        # not using all 10 classes)\n",
    "        self.output_layer = nn.Linear(nqubits, 10)\n",
    "\n",
    "        # This is the randomly initialised weights tensor for the quantum layer.\n",
    "        self.qnn_weights = torch.rand(nlayers * nqubits) * np.pi\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transform image array to a vector\n",
    "        x = self.flatten(x)\n",
    "        # scale pixel values to [0, 1] range\n",
    "        x = x / 255.0\n",
    "        # apply classical dimensionality reduction layer\n",
    "        x = self.reduction_layer(x)\n",
    "        # apply pi*tanh activation to put data into the range from -pi\n",
    "        # to pi\n",
    "        x = torch.tanh(x) * torch.pi\n",
    "        # apply the qnn layer to the input and weights.\n",
    "        # older versions of pennylane do not support batches,\n",
    "        # so we iterate through the batch and apply the qnn manually.\n",
    "        batch_size = x.size(0)\n",
    "        out = torch.empty((batch_size, nqubits), dtype=torch.float)\n",
    "        for batch_index in range(batch_size):\n",
    "            expval_tensors = qnn_layer(x[batch_index], self.qnn_weights)\n",
    "            expval_floats = [t.item() for t in expval_tensors]\n",
    "            out[batch_index] = torch.tensor(expval_floats)\n",
    "        x = out\n",
    "        # apply output layer to combine qnn outputs to 10 numbers\n",
    "        x = self.output_layer(x)\n",
    "        # just return outputs since we will use cross entropy loss in\n",
    "        # training\n",
    "        return x\n",
    "\n",
    "model = HybridClassifier().to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines function to run single epochs / passes of training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the below code is all used to train the defined model\n",
    "# and will be run when this file is loaded.\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(torch_device)\n",
    "        y = y.to(torch_device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    pred = None\n",
    "    X = None\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(torch_device)\n",
    "            y = y.to(torch_device)\n",
    "            pred = model(X)\n",
    "            predicted_label = torch.argmax(pred, dim=1)\n",
    "            correct += torch.count_nonzero(predicted_label == y)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    print(f\"Accuracy: {correct / (len(dataloader) * batch_size) * 100}\")\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will start the training process when executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Before training\\n--------------------------\")\n",
    "test_loop(test_dataloader, model, loss_fn)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loss = test_loop(test_dataloader, model, loss_fn)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time/60:.2f} minutes.\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
